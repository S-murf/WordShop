{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full Executer WordShop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "97c55aaef0cf420198fbd62f189c6225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cb849578d6614a23b0e9222224df5734",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f9ec49769ec647b1b198825e1ad5a928",
              "IPY_MODEL_6e66f8817d4d42a08287d6dc50e62da7"
            ]
          }
        },
        "cb849578d6614a23b0e9222224df5734": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9ec49769ec647b1b198825e1ad5a928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2111245e1abf44c2996594110af8e15a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1912529,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1912529,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_feaf223713dd48748fbf64e11594338b"
          }
        },
        "6e66f8817d4d42a08287d6dc50e62da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e457d8d111a74ff5afc683c62b2ba0bd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.91M/1.91M [00:00&lt;00:00, 6.70MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2333cccc5137448490e3fac3fdd265c6"
          }
        },
        "2111245e1abf44c2996594110af8e15a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "feaf223713dd48748fbf64e11594338b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e457d8d111a74ff5afc683c62b2ba0bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2333cccc5137448490e3fac3fdd265c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57a6dd48d2bf4b0284471eaff5ee10db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b14be02b722c4a579c00cc4d0a0b43f2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0ea51df5c7914e8e963e3f9ec3c7bf59",
              "IPY_MODEL_d7e1e9bf4046470c8aa87ce8eed82824"
            ]
          }
        },
        "b14be02b722c4a579c00cc4d0a0b43f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ea51df5c7914e8e963e3f9ec3c7bf59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_600d0ad6ed79415488f2f8a46586a9e5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 65,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 65,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49273e857fe547cf89f628b97f2e6e84"
          }
        },
        "d7e1e9bf4046470c8aa87ce8eed82824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9b044b0eae374a6e9948c4780ae3d2a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 65.0/65.0 [01:05&lt;00:00, 1.00s/B]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_20092cdb5d5a4e29bedad77f66b324ad"
          }
        },
        "600d0ad6ed79415488f2f8a46586a9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49273e857fe547cf89f628b97f2e6e84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b044b0eae374a6e9948c4780ae3d2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "20092cdb5d5a4e29bedad77f66b324ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f09123ddcf2458d8493b3d6aecbc087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0e13de98ea514d8e857e8d6c1ff6d55d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f780aeecad68473b8dda894410fd4384",
              "IPY_MODEL_ad6d2c8db24e4a549c794a14db8e6f19"
            ]
          }
        },
        "0e13de98ea514d8e857e8d6c1ff6d55d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f780aeecad68473b8dda894410fd4384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_609f5e4ad38447e398ae8230c6eb8115",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 86,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 86,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2b446b3519b4e53807954a3983a875b"
          }
        },
        "ad6d2c8db24e4a549c794a14db8e6f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f87e097c720e4b6ea66880e18d826c62",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 86.0/86.0 [00:00&lt;00:00, 125B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ebf9819965a24a818f30fdb0f74ad9f0"
          }
        },
        "609f5e4ad38447e398ae8230c6eb8115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2b446b3519b4e53807954a3983a875b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f87e097c720e4b6ea66880e18d826c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ebf9819965a24a818f30fdb0f74ad9f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26be970f3bd6489e96f4100e7be0d8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_74bc187826b9407a8127cf4fd9af7bd5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_761ac41d1d6544e9bb45c810a205a23c",
              "IPY_MODEL_8e5d4f49d66c4899a583e95ef4fa5542"
            ]
          }
        },
        "74bc187826b9407a8127cf4fd9af7bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "761ac41d1d6544e9bb45c810a205a23c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ac8b53ac437443095beb0d11740a797",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1142,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1142,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e772d5f9d8c54ab685623d02bd02abd3"
          }
        },
        "8e5d4f49d66c4899a583e95ef4fa5542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c47204f3a3bb44fb84e4e57456946a68",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.14k/1.14k [00:00&lt;00:00, 3.53kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1618e3ff63f842feb6366aac9c363bc4"
          }
        },
        "3ac8b53ac437443095beb0d11740a797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e772d5f9d8c54ab685623d02bd02abd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c47204f3a3bb44fb84e4e57456946a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1618e3ff63f842feb6366aac9c363bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23c7a838495247de9049d043528a0669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e4cef57ec68147f7979643e10ccbb82e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_68b5ab3851b54f52bb227a83f37e7333",
              "IPY_MODEL_58ad4df6e31046629253b3beb39c66de"
            ]
          }
        },
        "e4cef57ec68147f7979643e10ccbb82e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68b5ab3851b54f52bb227a83f37e7333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b655e804f82b4660a6f1935d7a9e4440",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2275437102,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2275437102,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b9edb59ca794942b4516a156662e923"
          }
        },
        "58ad4df6e31046629253b3beb39c66de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9335555cbb27436e93ec322036a7200c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.28G/2.28G [01:04&lt;00:00, 35.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da55cae7e0bc4f2cb2f7a50bc88de826"
          }
        },
        "b655e804f82b4660a6f1935d7a9e4440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b9edb59ca794942b4516a156662e923": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9335555cbb27436e93ec322036a7200c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da55cae7e0bc4f2cb2f7a50bc88de826": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUkswVS9WBSh"
      },
      "source": [
        "First, we load the pegasus paraphraser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCDd1YUKdy8d",
        "outputId": "ec55a031-2646-4766-9248-a3324e432c5e"
      },
      "source": [
        "!git clone https://github.com/google-research/pegasus\n",
        "%cd pegasus\n",
        "!export PYTHONPATH=.\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pegasus'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Total 175 (delta 0), reused 0 (delta 0), pack-reused 175\u001b[K\n",
            "Receiving objects: 100% (175/175), 354.16 KiB | 6.21 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n",
            "/content/pegasus\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.10.0)\n",
            "Collecting mock\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.18.5)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 11.7MB/s \n",
            "\u001b[?25hCollecting tensorflow-text==1.15.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/2c/57a6cc36f0ee1acf466a71048e2a0a2a01742eef505b3195d0fa27d87968/tensorflow_text-1.15.0rc0-cp36-cp36m-manylinux1_x86_64.whl (8.6MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6MB 13.1MB/s \n",
            "\u001b[?25hCollecting tensor2tensor==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/a6/0f6a95a5548cf9fbfed0bbec69959355bda594e85298ebbba5f808ca0fc5/tensor2tensor-1.15.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 44.0MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/7d/93c957b63baed31cd9660c49c393bf7f2eb5b24524c59311053efbb03af0/tfds_nightly-4.1.0.dev202012140107-py3-none-any.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 39.3MB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/ca/58e40e5077fa2a92004f398d705a288e958434f123938f4ce75ffe25b64b/tensorflow_gpu-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (411.0MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0MB 35kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score->-r requirements.txt (line 4)) (3.2.5)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/portalocker/\u001b[0m\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting tensorflow<1.16,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/64/7a19837dd54d3f53b1ce5ae346ab401dde9678e8f233220317000bfdb3e2/tensorflow-1.15.4-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 28kB/s \n",
            "\u001b[?25hCollecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/92/b80b922f08f222faca53c8d278e2e612192bc74b0e1f0db2f80a6ee46982/gevent-20.9.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 39.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.7.12)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (4.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (4.41.1)\n",
            "Collecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.1.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (7.0.0)\n",
            "Collecting mesh-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8b/553deb763ce8d00afb17debab7cb14a87b209cd4c6f0e8ecfc8d884cb12a/mesh_tensorflow-0.1.17-py3-none-any.whl (342kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 46.4MB/s \n",
            "\u001b[?25hCollecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 34.7MB/s \n",
            "\u001b[?25hCollecting bz2file\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (4.1.2.30)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (2.10.0)\n",
            "Collecting kfac\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/36/06fe2c757044bb51906fef231ac48cc5bf9a277fc9a8c7e1108d7e9e8cfd/kfac-0.2.3-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 43.8MB/s \n",
            "\u001b[?25hCollecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 34.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dopamine-rl in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.0.5)\n",
            "Collecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (0.17.3)\n",
            "Collecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (4.0.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->-r requirements.txt (line 9)) (0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->-r requirements.txt (line 9)) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->-r requirements.txt (line 9)) (3.12.4)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->-r requirements.txt (line 9)) (20.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->-r requirements.txt (line 9)) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->-r requirements.txt (line 9)) (0.25.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->-r requirements.txt (line 9)) (2.3)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->-r requirements.txt (line 9)) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->-r requirements.txt (line 9)) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 10)) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 10)) (0.36.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 10)) (1.34.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 10)) (1.12.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 10)) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2->-r requirements.txt (line 10)) (3.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from gevent->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (50.3.2)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 51.8MB/s \n",
            "\u001b[?25hCollecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d0/532e160c777b42f6f393f9de8c88abb8af6c892037c55e4d3a8a211324dd/greenlet-0.4.17-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.17.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (0.2.8)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gan->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.5.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (0.1.5)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly->-r requirements.txt (line 9)) (1.52.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly->-r requirements.txt (line 9)) (3.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 10)) (3.3.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor==1.15.0->-r requirements.txt (line 8)) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2->-r requirements.txt (line 10)) (3.1.1)\n",
            "Building wheels for collected packages: pypng, bz2file, gast\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp36-none-any.whl size=67162 sha256=f4a38d1376ea368e1ffcdee0290c6b624daaf0be28b8c816016815a7a723e906\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-cp36-none-any.whl size=6883 sha256=d3b428488d5729465b668e3975cd36b27b89fa96a69366b11adc8b88859b54eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=99a2212ef228e985b3a0b8cb8ca4ac97576f31d6738ab7a6dba8d3a3e87a0f70\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built pypng bz2file gast\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: mock, rouge-score, portalocker, sacrebleu, sentencepiece, tensorboard, tensorflow-estimator, gast, keras-applications, tensorflow, tensorflow-text, zope.event, zope.interface, greenlet, gevent, pypng, mesh-tensorflow, tensorflow-probability, tensorflow-gan, bz2file, kfac, gunicorn, tf-slim, tensor2tensor, tfds-nightly, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "  Found existing installation: tensorflow-probability 0.11.0\n",
            "    Uninstalling tensorflow-probability-0.11.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.11.0\n",
            "Successfully installed bz2file-0.98 gast-0.2.2 gevent-20.9.0 greenlet-0.4.17 gunicorn-20.0.4 keras-applications-1.0.8 kfac-0.2.3 mesh-tensorflow-0.1.17 mock-4.0.3 portalocker-2.0.0 pypng-0.0.20 rouge-score-0.0.4 sacrebleu-1.4.14 sentencepiece-0.1.94 tensor2tensor-1.15.0 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1 tensorflow-gan-2.0.0 tensorflow-gpu-1.15.2 tensorflow-probability-0.7.0 tensorflow-text-1.15.0rc0 tf-slim-1.1.0 tfds-nightly-4.1.0.dev202012140107 zope.event-4.5.0 zope.interface-5.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9vYLw5Jd-MI",
        "outputId": "d8882221-8ddc-44e1-b313-298e51e4030f"
      },
      "source": [
        "!pip install transformers==3.5.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2.23.0)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 16.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (20.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2019.12.20)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 34.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.0) (50.3.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=0e279a57b946faa40c589e8f6aa96a7a2917a4aa5460c08292fee051f6d66287\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "  Found existing installation: sentencepiece 0.1.94\n",
            "    Uninstalling sentencepiece-0.1.94:\n",
            "      Successfully uninstalled sentencepiece-0.1.94\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265,
          "referenced_widgets": [
            "97c55aaef0cf420198fbd62f189c6225",
            "cb849578d6614a23b0e9222224df5734",
            "f9ec49769ec647b1b198825e1ad5a928",
            "6e66f8817d4d42a08287d6dc50e62da7",
            "2111245e1abf44c2996594110af8e15a",
            "feaf223713dd48748fbf64e11594338b",
            "e457d8d111a74ff5afc683c62b2ba0bd",
            "2333cccc5137448490e3fac3fdd265c6",
            "57a6dd48d2bf4b0284471eaff5ee10db",
            "b14be02b722c4a579c00cc4d0a0b43f2",
            "0ea51df5c7914e8e963e3f9ec3c7bf59",
            "d7e1e9bf4046470c8aa87ce8eed82824",
            "600d0ad6ed79415488f2f8a46586a9e5",
            "49273e857fe547cf89f628b97f2e6e84",
            "9b044b0eae374a6e9948c4780ae3d2a3",
            "20092cdb5d5a4e29bedad77f66b324ad",
            "3f09123ddcf2458d8493b3d6aecbc087",
            "0e13de98ea514d8e857e8d6c1ff6d55d",
            "f780aeecad68473b8dda894410fd4384",
            "ad6d2c8db24e4a549c794a14db8e6f19",
            "609f5e4ad38447e398ae8230c6eb8115",
            "a2b446b3519b4e53807954a3983a875b",
            "f87e097c720e4b6ea66880e18d826c62",
            "ebf9819965a24a818f30fdb0f74ad9f0",
            "26be970f3bd6489e96f4100e7be0d8a7",
            "74bc187826b9407a8127cf4fd9af7bd5",
            "761ac41d1d6544e9bb45c810a205a23c",
            "8e5d4f49d66c4899a583e95ef4fa5542",
            "3ac8b53ac437443095beb0d11740a797",
            "e772d5f9d8c54ab685623d02bd02abd3",
            "c47204f3a3bb44fb84e4e57456946a68",
            "1618e3ff63f842feb6366aac9c363bc4",
            "23c7a838495247de9049d043528a0669",
            "e4cef57ec68147f7979643e10ccbb82e",
            "68b5ab3851b54f52bb227a83f37e7333",
            "58ad4df6e31046629253b3beb39c66de",
            "b655e804f82b4660a6f1935d7a9e4440",
            "7b9edb59ca794942b4516a156662e923",
            "9335555cbb27436e93ec322036a7200c",
            "da55cae7e0bc4f2cb2f7a50bc88de826"
          ]
        },
        "id": "8Q7u0dLyVwnj",
        "outputId": "66731bb6-21b6-47d2-a2c4-8c8c9296abad"
      },
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "def get_response(input_text,num_return_sequences,num_beams):\n",
        "    batch = tokenizer.prepare_seq2seq_batch([input_text],truncation=True,padding='longest',max_length=60).to(torch_device)\n",
        "    translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    return tgt_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97c55aaef0cf420198fbd62f189c6225",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1912529.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57a6dd48d2bf4b0284471eaff5ee10db",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=65.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f09123ddcf2458d8493b3d6aecbc087",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=86.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26be970f3bd6489e96f4100e7be0d8a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1142.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23c7a838495247de9049d043528a0669",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2275437102.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o4lJcv5d-UE",
        "outputId": "5818229f-d27d-4140-c63a-abb6e08f871a"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('cmudict')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MMun5MYV5kF"
      },
      "source": [
        "Next, we import the procrustean alliteration paraphraser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6d5lW3Dd-Xj"
      },
      "source": [
        "class RuleBoundsInterface:\n",
        "    \"\"\"This interface is used to define different properties of a rhetorical figure generating algorithm.\n",
        "These properties will create a ruleset for a rhectorical figure, allowing the algorithm to produce results relevant\n",
        "to the user-request.\"\"\"\n",
        "\n",
        "    def evaluate(self, tokenlist, replacementquota):\n",
        "        \"\"\"Returns a dataset containing the best application of the rule to the original tokenlist using the proportion specified. This also means\n",
        "        that certain conditions will have no return value.\"\"\"\n",
        "        pass\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17UCKMlxd-an"
      },
      "source": [
        "class LanguageBoundsInterface:\n",
        "    \"\"\"This interface is used to define different properties and implementation of a language.\n",
        "These properties will create a ruleset for a language, which will be the bounds an algorithm\n",
        "can work in the given context.\"\"\"\n",
        "\n",
        "    ########## Variables ##########\n",
        "\n",
        "    _NULL_PHENOME_INDICATOR = None # Phenome representation of an unknown phenome\n",
        "    _SIMILARITY_THRESHOLD = 0.2 # The threshold that must be passed for a word to be considered similar. Scaled from 0-1.\n",
        "    MULTI_TOKEN_INDICATOR = None # Character used to identify when a token has multiple words. This functionality is specific to a corpus. Must be changed if corpus is changed.\n",
        "    vowelphenomes = None # Contains all phenomes that produce vowel-related sounds for this language.\n",
        "    \n",
        "    ###############################\n",
        "\n",
        "    ########## Constructor ##########\n",
        "\n",
        "    def __init__(self, sensitivity):\n",
        "        self._SIMILARITY_THRESHOLD = sensitivity\n",
        "\n",
        "    #################################\n",
        "\n",
        "    def getphenomes(self, arg):\n",
        "        \"\"\"Returns all phenome-lists related to the token.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def hypernyms(self, arg):\n",
        "        \"\"\"Returns all hypernyms related to the token. ('context' is the representation of the phrase in collection form.)\"\"\"\n",
        "        pass\n",
        "\n",
        "    def hyponyms(self, arg):\n",
        "        \"\"\"Returns all hyponyms related to the token. ('context' is the representation of the phrase in collection form.)\"\"\"\n",
        "        pass\n",
        "\n",
        "    def messagefail(self, input):\n",
        "        \"\"\"Produces the fail message to print to users in this language if the process cannot return a value.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def messageonlyresult(self, arg):\n",
        "        \"\"\"Produces a indicator message if only one result was possible from the input parameters given.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def messagetopresult(self, resultlen, requestedresultcount):\n",
        "        \"\"\"Produces the top 'x' results message to users in this language if the process has multiple results.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def similarity(self, arg, arg2):\n",
        "        \"\"\"Returns a token similarity score based on language-based weights. Used for determining optimal replacement for\n",
        "        contexts.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def split(self, arg):\n",
        "        \"\"\"Returns an ordered list of tokens, split at delimiters based off of the the language context settings.\"\"\"\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HxsPqDOd-dW"
      },
      "source": [
        "from enum import Enum\n",
        "from nltk import RegexpTokenizer\n",
        "from nltk.corpus import cmudict\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "\n",
        "class AmericanEnglishLangContext(LanguageBoundsInterface):\n",
        "    \"\"\"Defines the properties and implementation of standard American English.\"\"\"\n",
        "\n",
        "    ########## Variables ##########\n",
        "    \n",
        "    _cmu = cmudict.dict() # Pretrained phenome generation model. Created outside of methods because it is used over iteration(s) and is expensive to generate; TREAT THIS VALUE AS AN IMMUTABLE.\n",
        "    _MULTI_TOKEN_INDICATOR = \"_\" # Character used to identify when a token has multiple words. This functionality is specific to a corpus. Must be changed if corpus is changed.\n",
        "    _NULL_PHENOME_INDICATOR = \"*NONE*\" # Used by algorithm to indicate if a corressponding phemone could not be found for a token\n",
        "    _SIMILARITY_THRESHOLD = 0.2 # The threshold that must be passed for a word to be considered similar. Scaled from 0-1.\n",
        "    vowelphenomes = [\"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\",\n",
        "                    \"AX\", \"AXR\", \"EH\", \"ER\", \"EY\", \"IH\",\n",
        "                    \"IX\", \"IY\", \"OW\", \"OY\",\"UH\", \"UW\", \"UX\"] # Contains all phenomes that produce vowel-related sounds for this language.\n",
        "    \n",
        "    ###############################\n",
        "    def _getproperformattype(self, unformattoken):\n",
        "        \"\"\"Used to parse through the Wordnet sysnet-token return value to retrieve only relevant sections. Currently the only returns the word.\n",
        "        In future implementations, this function may not be needed if the corpus has a function to return only the word as a string.\"\"\"\n",
        "\n",
        "        name, junk = unformattoken.name().split(\".\", 1);\n",
        "        return name\n",
        "\n",
        "    def _getproperhandlemissingphenome(self, unknowntoken):\n",
        "        \"\"\"Takes a unknown-phenome (a token which could not be evaluated by CMUdict) and attempts to generate a phenome. If CMUdict or\n",
        "        Wordnet implementation is changed this function MUST be changed.\"\"\"\n",
        "\n",
        "        finaleval = []\n",
        "\n",
        "        # After various testing, it has been determined that calculating for two letters yields the most consistent results for unknown phenomes.\n",
        "        tokenlen = len(unknowntoken)\n",
        "        if tokenlen is 0:\n",
        "            finaleval.append([self._NULL_PHENOME_INDICATOR])\n",
        "        elif tokenlen is 1:\n",
        "            finaleval.append([unknowntoken.upper()]) # The letter IS the phenome\n",
        "        else:\n",
        "            relevant = unknowntoken[:2] # get first two chars\n",
        "            finalattempt = self._cmu.get(relevant, None)\n",
        "\n",
        "            if finalattempt is None: # No possible phenome can be generated by this algorithm\n",
        "                finaleval.append([self._NULL_PHENOME_INDICATOR])\n",
        "            elif finalattempt is list:\n",
        "                finaleval.append(finalattempt)\n",
        "            else:  # 'finalattempt' is guareenteed to only be of type NONE, list, or list[list].\n",
        "                finaleval.extend(finalattempt) # flatten list; tis step is necessary to maintain parsability\n",
        "\n",
        "        return finaleval\n",
        "\n",
        "    def _getproperhandlemultitoken(self, multitoken):\n",
        "        \"\"\"Takes a multi-word (a token with words seperated by '_' by Wordnet) and breaks it down into a format that can be evaluated by the CMUdict. If CMUdict or\n",
        "        Wordnet implementation is changed this function MUST be changed.\"\"\"\n",
        "\n",
        "        finaleval = []\n",
        "        individualtokens = multitoken.split(self._MULTI_TOKEN_INDICATOR)\n",
        "\n",
        "        for token in individualtokens: # evaluate each token phenome indiviually; then represent multitoken for EACH phenome calculated, when returned to scanning.\n",
        "            phenome = self._cmu.get(token.lower(), None)\n",
        "               \n",
        "            if phenome is list:\n",
        "                finaleval.append(phenome)\n",
        "\n",
        "            else: # 'phenome' is guareenteed to only be of type NONE, list, or list[list].\n",
        "                if phenome is None:\n",
        "                    phenome = self._getproperhandlemissingphenome(token)\n",
        "                    \n",
        "                finaleval.extend(phenome) # flatten list; this step is necessary to maintain parsability\n",
        "\n",
        "        return finaleval\n",
        "\n",
        "    def getphenomes(self, arg):\n",
        "        \"\"\"Returns all phenome-lists related to the token. ('context' is the representation of the phrase in collection form.)\"\"\"\n",
        "\n",
        "        # uses CMUdict as the core processing algorithm. If CMUdict fails to find a match the function will predict a possible phenome for the token.\n",
        "        # This function is guareenteed to return a value.\n",
        "\n",
        "        generatephenome = self._cmu.get(arg.lower(), None) # _cmu is defined globally above in \"VARIABLES\" section. Treat as an immutable.\n",
        "        if generatephenome is None:\n",
        "            if arg.__contains__(self._MULTI_TOKEN_INDICATOR): # _MULTI_TOKEN_INDICATOR is defined globally above in \"VARIABLES\" section. Treat as an immutable.\n",
        "                generatephenome = self._getproperhandlemultitoken(arg)\n",
        "\n",
        "            else: # token is unknown by CMUdict\n",
        "                generatephenome = self._getproperhandlemissingphenome(arg)\n",
        "\n",
        "        # When multiple phenomes exist for same word, a list[list[str]] is generated\n",
        "        return generatephenome\n",
        "\n",
        "    def hypernyms(self, context, arg):\n",
        "        \"\"\"Returns all hypernyms related to the token. ('context' is the representation of the phrase in collection form.)\"\"\"\n",
        "\n",
        "        # This function assumes the use of Wordnet. If Wordnet implementation changes, this function MUST change.\n",
        "\n",
        "        eval = None\n",
        "        interpretation = lesk(context, arg)\n",
        "        if interpretation is not None:\n",
        "            eval = map(self._getproperformattype, interpretation.hypernyms())\n",
        "\n",
        "        return eval\n",
        "\n",
        "    def hyponyms(self, context, arg):\n",
        "        \"\"\"Returns all hyponyms related to the token.\"\"\"\n",
        "\n",
        "        # This function assumes the use of Wordnet. If Wordnet implementation changes, this function MUST change.\n",
        "\n",
        "        eval = None\n",
        "        interpretation = lesk(context, arg)\n",
        "        if interpretation is not None:\n",
        "            eval = map(self._getproperformattype, interpretation.hyponyms())\n",
        "\n",
        "        return eval\n",
        "\n",
        "    def messagefail(self, input):\n",
        "        \"\"\"Produces the fail message to print to users in this language if the process cannot return a value.\"\"\"\n",
        "        built = \" \".join(input)\n",
        "        return (\"Your input: '\" + built + \"' was not able to be parsed under the conditions you desired. Please try new conditions or try a new phrase.\")\n",
        "\n",
        "    def messageonlyresult(self, arg):\n",
        "        \"\"\"Produces a indicator message if only one result was possible from the input parameters given.\"\"\"\n",
        "        return (\"This is the only result processed from the given input:\\n\" + arg)\n",
        "    \n",
        "    def messagetopresult(self, resultlen, requestedresultcount):\n",
        "        \"\"\"Produces the top 'x' results message to users in this language if the process has multiple results.\"\"\"\n",
        "        if resultlen < requestedresultcount:\n",
        "            return (\"Top \" + str(resultlen) + \" result(s):\\n\")\n",
        "        else:\n",
        "            return (\"Top \" + str(requestedresultcount) + \" result(s):\\n\")\n",
        "\n",
        "    def similarity(self, contextclues, arg1, arg2):\n",
        "        \"\"\"Returns a key-value pair for scoring similarity. [0] a bool that determines if the word is similar enough to satisfy language criteria\n",
        "        and the score associated with the evaluation.\"\"\"\n",
        "\n",
        "        # This function assumes the use of Wordnet. If Wordnet implementation changes, this function MUST change.\n",
        "\n",
        "        evaluation = False\n",
        "        score = 0\n",
        "\n",
        "        if arg1 is arg2:\n",
        "            evaluation = True\n",
        "            score = self._SIMILARITY_THRESHOLD # Penalizing score to prevent paraphrases from returning themselves\n",
        "\n",
        "        else:\n",
        "            contextA = lesk(contextclues, arg1)\n",
        "            contextB = lesk(contextclues, arg2)\n",
        "\n",
        "            if contextA and contextB: # Otherwise score will stay zero\n",
        "                score = contextA.path_similarity(contextB)\n",
        "\n",
        "                if score is not None and self._SIMILARITY_THRESHOLD <= score:\n",
        "                    evaluation = True\n",
        "\n",
        "        return (evaluation, score)\n",
        "\n",
        "    def split(self, arg):\n",
        "        # Returns all non-whitespace tokens.\n",
        "        return RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+').tokenize(arg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3sHYdUdd-f6"
      },
      "source": [
        "class Mapper:\n",
        "    \"\"\"This module takes a user-input string and chunks it into formatted tuples which\n",
        "contains relevant data for all essential pieces of string.\"\"\"\n",
        "\n",
        "    def maptolist(self, arg, langbound):\n",
        "        \"\"\"Returns formatted-tuple containing relevant processing data for each token.\"\"\" #update\n",
        "        return self._internalmap(arg, langbound)\n",
        "\n",
        "    def _internalmap(self, arg, langbound):\n",
        "        # Internally seperates the string into relevant tokens. # update\n",
        "        return langbound.split(arg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AC3IQ0dd-iI"
      },
      "source": [
        "class AlliterationRuleContext(RuleBoundsInterface):\n",
        "    \"\"\"Defines the properties and rules of the alliteration rhetorical figure.\"\"\"\n",
        "\n",
        "    def _applyrule(self, sourcedata, tokentargetcount, langbound):\n",
        "        \"\"\"Trim interal-map token list to only retain tokens that constrain to the alliteration ruleset.\"\"\"\n",
        "\n",
        "        phenomeselect = []\n",
        "        for phenomeset in sourcedata:\n",
        "            if phenomeset not in langbound.vowelphenomes:\n",
        "                #run proportion algorithm\n",
        "                setrep = sourcedata[phenomeset]\n",
        "                if tokentargetcount <= len(setrep):\n",
        "                    phenomeselect.append(sourcedata[phenomeset])\n",
        "\n",
        "        # Return selection\n",
        "        if not phenomeselect:\n",
        "            return None\n",
        "        else:\n",
        "            return phenomeselect\n",
        "\n",
        "    def _applyscan(self, sourcematrix, langbound):\n",
        "        \"\"\"Scan a token-matrix and return a dataset that holds information on the phenome frequency of alliteration\n",
        "        in the matrix.\"\"\"\n",
        "\n",
        "        dataset = {} # Dicitonary is of type 'char' -> dict{int -> list[str]}'\n",
        "\n",
        "        for index, item in enumerate(sourcematrix): # going through each token\n",
        "            for content in item: # going through synonym content of each token\n",
        "\n",
        "                phenomelists = self._getsourcephenome(content, langbound) # generate phenomes for alliteration evaluation\n",
        "                for phenomes in phenomelists: # going through each phenome list for token (some tokes may have multiple pronounciations)\n",
        "\n",
        "                    relevantphenome = phenomes[0] # use the FIRST phenome because this is alliteration\n",
        "\n",
        "                    if dataset.get(relevantphenome, None) is None: # if letter has NOT been scanned previously, create an entry\n",
        "                       dataset[ relevantphenome ] = {} # Dictionary will contain key-value pairs corresponding to the index and the list of words available.\n",
        "                       dataset[ relevantphenome ] [index] = [content]\n",
        "\n",
        "                    else:\n",
        "                        if dataset[ relevantphenome ].get(index, None) is None: # if an entry for THIS index has NOT been created, create one.\n",
        "                            dataset[ relevantphenome ] [index] = [content]\n",
        "                        else:\n",
        "                            if content not in dataset[ relevantphenome ] [index]:\n",
        "                                dataset[ relevantphenome ] [index].append(content)\n",
        "\n",
        "                       \n",
        "        return dataset\n",
        "\n",
        "    def _getsourcephenome(self, evaltoken, langbound):\n",
        "        \"\"\"Returns a phenome value for a string-token.\"\"\"\n",
        "\n",
        "        phenomeform = langbound.getphenomes(evaltoken)\n",
        "        return phenomeform\n",
        "\n",
        "\n",
        "    def _getrelevantsynonyms(self, tokenlist, sourcetoken, langbound):\n",
        "        \"\"\"Returns a token-list of the original context and synonyms that are relevant, if applicable.\"\"\"\n",
        "\n",
        "        relevant = [sourcetoken] # original token is always first within the collection\n",
        "\n",
        "        # Add all relevant synonyms to evaluation list as strings\n",
        "\n",
        "        hypernyms = langbound.hypernyms(tokenlist, sourcetoken)\n",
        "        hyponyms = langbound.hyponyms(tokenlist, sourcetoken)\n",
        "\n",
        "        if hypernyms is not None:\n",
        "            relevant.extend(hypernyms)\n",
        "        if hyponyms is not None:\n",
        "            relevant.extend(hyponyms)\n",
        "\n",
        "        return relevant\n",
        "\n",
        "    def _internalmap(self, tokenlist, langbound):\n",
        "        \"\"\"Map relevant replacement tokens to a matrix. This return token-matrix will have a one-to-to corresspondence\n",
        "        to the passed tokenlist argument.\"\"\"\n",
        "\n",
        "        replacements = []\n",
        "        for token in tokenlist:\n",
        "            similar = self._getrelevantsynonyms(tokenlist, token, langbound)\n",
        "            replacements.append(similar)\n",
        "\n",
        "        return replacements\n",
        "\n",
        "    def evaluate(self, tokenlist, replacementquota, langbound):\n",
        "        if replacementquota < 1: # Nothing will be applied to the target list. Do not process.\n",
        "            return None\n",
        "\n",
        "        # Map and chart data for rule application\n",
        "        preprocess = self._internalmap(tokenlist, langbound)\n",
        "        process = self._applyscan(preprocess, langbound)\n",
        "\n",
        "        # Apply rule and return data\n",
        "        postprocess = self._applyrule(process, replacementquota, langbound)\n",
        "        return postprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJTKvRN4tJhm"
      },
      "source": [
        "class AssonanceRuleContext(RuleBoundsInterface):\r\n",
        "    \"\"\"Defines the properties and rules of the assonance rhetorical figure.\"\"\"\r\n",
        "\r\n",
        "    def _applyrule(self, sourcedata, tokentargetcount, langbound):\r\n",
        "        \"\"\"Trim interal-map token list to only retain tokens that constrain to the assonance ruleset.\"\"\"\r\n",
        "\r\n",
        "        phenomeselect = []\r\n",
        "        for phenomeset in sourcedata:\r\n",
        "            if phenomeset in langbound.vowelphenomes:\r\n",
        "                #run proportion algorithm\r\n",
        "                setrep = sourcedata[phenomeset]\r\n",
        "                if tokentargetcount <= len(setrep):\r\n",
        "                    phenomeselect.append(sourcedata[phenomeset])\r\n",
        "\r\n",
        "        # Return selection\r\n",
        "        if not phenomeselect:\r\n",
        "            return None\r\n",
        "        else:\r\n",
        "            return phenomeselect\r\n",
        "\r\n",
        "    def _applyscan(self, sourcematrix, langbound):\r\n",
        "        \"\"\"Scan a token-matrix and return a dataset that holds information on the phenome frequency of assonance\r\n",
        "        in the matrix.\"\"\"\r\n",
        "\r\n",
        "        dataset = {} # Dicitonary is of type 'char' -> dict{int -> list[str]}'\r\n",
        "\r\n",
        "        for index, item in enumerate(sourcematrix): # going through each token\r\n",
        "            for content in item: # going through synonym content of each token\r\n",
        "\r\n",
        "                phenomelists = self._getsourcephenome(content, langbound) # generate phenomes for assonance evaluation\r\n",
        "                for phenomes in phenomelists: # going through each phenome list for token (some tokes may have multiple pronounciations)\r\n",
        "\r\n",
        "                    relevantphenome = phenomes[0] # use the FIRST phenome because this is assonance\r\n",
        "\r\n",
        "                    if dataset.get(relevantphenome, None) is None: # if letter has NOT been scanned previously, create an entry\r\n",
        "                       dataset[ relevantphenome ] = {} # Dictionary will contain key-value pairs corresponding to the index and the list of words available.\r\n",
        "                       dataset[ relevantphenome ] [index] = [content]\r\n",
        "\r\n",
        "                    else:\r\n",
        "                        if dataset[ relevantphenome ].get(index, None) is None: # if an entry for THIS index has NOT been created, create one.\r\n",
        "                            dataset[ relevantphenome ] [index] = [content]\r\n",
        "                        else:\r\n",
        "                            if content not in dataset[ relevantphenome ] [index]:\r\n",
        "                                dataset[ relevantphenome ] [index].append(content)\r\n",
        "                                \r\n",
        "        return dataset\r\n",
        "\r\n",
        "    def _getsourcephenome(self, evaltoken, langbound):\r\n",
        "        \"\"\"Returns a phenome value for a string-token using CMUdict as the core processing algorithm. If CMUdict fails to find a match\r\n",
        "        the function will predict a possible phenome for the token. This function is guareenteed to return a value.\"\"\"\r\n",
        "\r\n",
        "        phenomeform = langbound.getphenomes(evaltoken)\r\n",
        "        return phenomeform\r\n",
        "\r\n",
        "\r\n",
        "    def _getrelevantsynonyms(self, tokenlist, sourcetoken, langbound):\r\n",
        "        \"\"\"Returns a token-list of the original context and synonyms that are relevant, if applicable.\"\"\"\r\n",
        "\r\n",
        "        relevant = [sourcetoken] # original token is always first within the collection\r\n",
        "\r\n",
        "        # Add all relevant synonyms to evaluation list as strings\r\n",
        "\r\n",
        "        hypernyms = langbound.hypernyms(tokenlist, sourcetoken)\r\n",
        "        hyponyms = langbound.hyponyms(tokenlist, sourcetoken)\r\n",
        "\r\n",
        "        if hypernyms is not None:\r\n",
        "            relevant.extend(hypernyms)\r\n",
        "        if hyponyms is not None:\r\n",
        "            relevant.extend(hyponyms)\r\n",
        "\r\n",
        "        return relevant\r\n",
        "\r\n",
        "    def _internalmap(self, tokenlist, langbound):\r\n",
        "        \"\"\"Map relevant replacement tokens to a matrix. This return token-matrix will have a one-to-to corresspondence\r\n",
        "        to the passed tokenlist argument.\"\"\"\r\n",
        "\r\n",
        "        replacements = []\r\n",
        "        for token in tokenlist:\r\n",
        "            similar = self._getrelevantsynonyms(tokenlist, token, langbound)\r\n",
        "            replacements.append(similar)\r\n",
        "\r\n",
        "        return replacements\r\n",
        "\r\n",
        "    def evaluate(self, tokenlist, replacementquota, langbound):\r\n",
        "        if replacementquota < 1: # Nothing will be applied to the target list. Do not process.\r\n",
        "            return None\r\n",
        "\r\n",
        "        # Map and chart data for rule application\r\n",
        "        preprocess = self._internalmap(tokenlist, langbound)\r\n",
        "        process = self._applyscan(preprocess, langbound)\r\n",
        "\r\n",
        "        # Apply rule and return data\r\n",
        "        postprocess = self._applyrule(process, replacementquota, langbound)\r\n",
        "        return postprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ESso9tJtL94"
      },
      "source": [
        "class RhymeRuleContext(RuleBoundsInterface):\r\n",
        "    \"\"\"Defines the properties and rules of the rhyme rhetorical figure.\"\"\"\r\n",
        "\r\n",
        "    def _applyrule(self, sourcedata, tokentargetcount, langbound):\r\n",
        "        \"\"\"Trim interal-map token list to only retain tokens that constrain to the rhyme ruleset.\"\"\"\r\n",
        "\r\n",
        "        phenomeselect = []\r\n",
        "        for phenomeset in sourcedata:\r\n",
        "            #run proportion algorithm\r\n",
        "            setrep = sourcedata[phenomeset]\r\n",
        "            if tokentargetcount <= len(setrep):\r\n",
        "                phenomeselect.append(sourcedata[phenomeset])\r\n",
        "               \r\n",
        "        # Return selection\r\n",
        "        if not phenomeselect:\r\n",
        "            return None\r\n",
        "        else:\r\n",
        "            return phenomeselect\r\n",
        "\r\n",
        "    def _applyscan(self, sourcematrix, langbound):\r\n",
        "        \"\"\"Scan a token-matrix and return a dataset that holds information on the phenome frequency of rhyme\r\n",
        "        in the matrix.\"\"\"\r\n",
        "\r\n",
        "        dataset = {} # Dicitonary is of type 'char' -> dict{int -> list[str]}'\r\n",
        "\r\n",
        "        for index, item in enumerate(sourcematrix): # going through each token\r\n",
        "            for content in item: # going through synonym content of each token\r\n",
        "\r\n",
        "                phenomelists = self._getsourcephenome(content, langbound) # generate phenomes for rhyme evaluation\r\n",
        "                for phenomes in phenomelists: # going through each phenome list for token (some tokes may have multiple pronounciations)\r\n",
        "\r\n",
        "                    relevantphenome = phenomes[len(phenomes) - 1] # use the LAST phenome because this is rhyme\r\n",
        "\r\n",
        "                    if dataset.get(relevantphenome, None) is None: # if letter has NOT been scanned previously, create an entry\r\n",
        "                       dataset[ relevantphenome ] = {} # Dictionary will contain key-value pairs corresponding to the index and the list of words available.\r\n",
        "                       dataset[ relevantphenome ] [index] = [content]\r\n",
        "\r\n",
        "                    else:\r\n",
        "                        if dataset[ relevantphenome ].get(index, None) is None: # if an entry for THIS index has NOT been created, create one.\r\n",
        "                            dataset[ relevantphenome ] [index] = [content]\r\n",
        "                        else:\r\n",
        "                            if content not in dataset[ relevantphenome ] [index]:\r\n",
        "                                dataset[ relevantphenome ] [index].append(content)\r\n",
        "                                \r\n",
        "        return dataset\r\n",
        "\r\n",
        "    def _getsourcephenome(self, evaltoken, langbound):\r\n",
        "        \"\"\"Returns a phenome value for a string-token.\"\"\"\r\n",
        "\r\n",
        "        phenomeform = langbound.getphenomes(evaltoken)\r\n",
        "        return phenomeform\r\n",
        "\r\n",
        "\r\n",
        "    def _getrelevantsynonyms(self, tokenlist, sourcetoken, langbound):\r\n",
        "        \"\"\"Returns a token-list of the original context and synonyms that are relevant, if applicable.\"\"\"\r\n",
        "\r\n",
        "        relevant = [sourcetoken] # original token is always first within the collection\r\n",
        "\r\n",
        "        # Add all relevant synonyms to evaluation list as strings\r\n",
        "\r\n",
        "        hypernyms = langbound.hypernyms(tokenlist, sourcetoken)\r\n",
        "        hyponyms = langbound.hyponyms(tokenlist, sourcetoken)\r\n",
        "\r\n",
        "        if hypernyms is not None:\r\n",
        "            relevant.extend(hypernyms)\r\n",
        "        if hyponyms is not None:\r\n",
        "            relevant.extend(hyponyms)\r\n",
        "\r\n",
        "        return relevant\r\n",
        "\r\n",
        "    def _internalmap(self, tokenlist, langbound):\r\n",
        "        \"\"\"Map relevant replacement tokens to a matrix. This return token-matrix will have a one-to-to corresspondence\r\n",
        "        to the passed tokenlist argument.\"\"\"\r\n",
        "\r\n",
        "        replacements = []\r\n",
        "        for token in tokenlist:\r\n",
        "            similar = self._getrelevantsynonyms(tokenlist, token, langbound)\r\n",
        "            replacements.append(similar)\r\n",
        "\r\n",
        "        return replacements\r\n",
        "\r\n",
        "    def evaluate(self, tokenlist, replacementquota, langbound):\r\n",
        "        if replacementquota < 1: # Nothing will be applied to the target list. Do not process.\r\n",
        "            return None\r\n",
        "\r\n",
        "        # Map and chart data for rule application\r\n",
        "        preprocess = self._internalmap(tokenlist, langbound)\r\n",
        "        process = self._applyscan(preprocess, langbound)\r\n",
        "\r\n",
        "        # Apply rule and return data\r\n",
        "        postprocess = self._applyrule(process, replacementquota, langbound)\r\n",
        "        return postprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geE3mDZHd-kb"
      },
      "source": [
        "import heapq\n",
        "\n",
        "class Analyzer:\n",
        "    \"\"\"Takes a rule-output and applies it to the target-input while attempting to retain the original meaning of the input as much as possible.\"\"\"\n",
        "\n",
        "    def _checkaccuracy(self, bestfitdata):\n",
        "        \"\"\"Takes a bestfitted output-slice and scores it on its accuracy when compared to original sentence.\"\"\"\n",
        "\n",
        "        if not bestfitdata:\n",
        "            return None\n",
        "        elif len(bestfitdata) is 1:\n",
        "            return bestfitdata[0] # only one possible return\n",
        "        else:\n",
        "            \n",
        "            accuracyheap = []\n",
        "            for collection in bestfitdata:\n",
        "                accuracy = 0 # reset every iteration\n",
        "                for item in collection:\n",
        "                    accuracy += item[0] # the first value in every tuple is expected to be accuracy score.\n",
        "\n",
        "                heapq.heappush(accuracyheap, (accuracy, collection))\n",
        "\n",
        "            return heapq.nlargest(len(accuracyheap) - 1, accuracyheap)[0][1] # contains all values in highest to lowest order, but only returns highest\n",
        "\n",
        "    def _checkbestfit(self, input, outputslice, langbound, replacementquota):\n",
        "        \"\"\"Takes a index to word-list mapping and selects the word from the list that retains the most sentence meaning. If no word applies, there will be no word\n",
        "        replacement for that slot.\"\"\"\n",
        "\n",
        "        bestfitinorder = [] # Used as a heapq to sort by highest scores.\n",
        "        for index in outputslice: # each replacement index in dict\n",
        "\n",
        "            tokenselect = None\n",
        "            highestscore = 0 # Used to select best word, resets on every changed index.\n",
        "\n",
        "            for token in outputslice[index]: # each word in replacement index\n",
        "                compareto = input[index] #corressponding index in original input\n",
        "                score = langbound.similarity(input, compareto, token)\n",
        "\n",
        "                if score[0] and highestscore < score[1]: # Does satisfy similarity criteria?\n",
        "                    tokenselect = token\n",
        "                    highestscore = score[1]\n",
        "\n",
        "            if tokenselect: # After evaluation, check if there are any valid tokens to use.\n",
        "                heapq.heappush(bestfitinorder, (highestscore, index, tokenselect)) # Acts as a priority queue and orders values from lowest to highest\n",
        "\n",
        "        # Returning\n",
        "        if len(bestfitinorder) < replacementquota: # Not enough values to return, replacements are invalid\n",
        "            return None\n",
        "        else:\n",
        "            return heapq.nlargest(int(replacementquota), bestfitinorder)\n",
        "\n",
        "    def _construct(self, originalinput, tokenchain, langbound, requestedresults): # add variable top X accurate results \n",
        "        \"\"\"Converts a token list and bestfit data into a readable string.\"\"\"\n",
        "\n",
        "        convergence = []\n",
        "        result = \" \"\n",
        "        empty = \" \" # placeholder to perform syntax operations on\n",
        "\n",
        "        if tokenchain is None:\n",
        "            #result = langbound.messagefail(originalinput)\n",
        "            result = \"\"\n",
        "\n",
        "        else:\n",
        "            convergence = self._replacetokens(originalinput, tokenchain)\n",
        "            #result = langbound.messageonlyresult(empty.join(convergence))\n",
        "            result = empty.join(convergence)\n",
        "            #if type(result) == str:\n",
        "            result = result.replace(\"_\",\" \")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _internalmap(self, originalinput, ruleoutput, langbound, replacementquota):\n",
        "        \"\"\"Maps each rule-output dictionary to be checked for bestfit.\"\"\"\n",
        "\n",
        "        mapping = []\n",
        "\n",
        "        if ruleoutput is not None:\n",
        "            for outs in ruleoutput: # Iterate through each output dict avaliable\n",
        "                trimmed = self._checkbestfit(originalinput, outs, langbound, replacementquota)\n",
        "                if trimmed is not None:\n",
        "                    mapping.append(trimmed)\n",
        "\n",
        "        return mapping\n",
        "\n",
        "    def _replacetokens(self, originalinput, tokenchain):\n",
        "        \"\"\"Replaces tokens at targeted indicies in the original input from tokenchain, and returns a list of the results. This algorithm expects tokenchain to be a list.\"\"\"\n",
        "\n",
        "        for item in tokenchain: # Tuples containing (score, index, token)\n",
        "            originalinput[item[1]] = item[2] # works because indicies were marked from beginning of process\n",
        "\n",
        "        return originalinput # this is after modifications\n",
        "            \n",
        "    def analyze(self, originalinput, ruleoutput, langbound, replacementquota, topresults = 1):\n",
        "        \"\"\"Applies a rule-output to a target input and attempts to retain input meaning. Rule output is expected to be in (float, list[ dict(int, list[str]) ]) format.\"\"\"\n",
        "\n",
        "        preprocess = self._internalmap(originalinput, ruleoutput, langbound, replacementquota)\n",
        "        process = self._checkaccuracy(preprocess)\n",
        "        postprocess = self._construct(originalinput, process, langbound, topresults)\n",
        "        return postprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HibXpa9Waszl"
      },
      "source": [
        "We define three methods for use in the final integrated function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P5mah6Od-sE"
      },
      "source": [
        "def alliteration(arg, proportion, sensitivity):\n",
        "    langcontext = AmericanEnglishLangContext(sensitivity)\n",
        "    mapsection = Mapper()\n",
        "    rule = AlliterationRuleContext()\n",
        "    interpreter = Analyzer()\n",
        "    mappedtokens = mapsection.maptolist(arg, langcontext)\n",
        "\n",
        "    # Calculate exact proportion\n",
        "    calcd = len(mappedtokens)\n",
        "    if proportion < 1:\n",
        "        calcd *= proportion\n",
        "        calcd = round(calcd)\n",
        "\n",
        "    applied = rule.evaluate(mappedtokens, calcd, langcontext)\n",
        "    finalresult = interpreter.analyze(mappedtokens, applied, langcontext, calcd)\n",
        "\n",
        "    return finalresult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSxu2rG_xJh0"
      },
      "source": [
        "def assonance(arg, proportion, sensitivity):\n",
        "    langcontext = AmericanEnglishLangContext(sensitivity)\n",
        "    mapsection = Mapper()\n",
        "    rule = AssonanceRuleContext()\n",
        "    interpreter = Analyzer()\n",
        "    mappedtokens = mapsection.maptolist(arg, langcontext)\n",
        "\n",
        "    # Calculate exact proportion\n",
        "    calcd = len(mappedtokens)\n",
        "    if proportion < 1:\n",
        "        calcd *= proportion\n",
        "        calcd = round(calcd)\n",
        "\n",
        "    applied = rule.evaluate(mappedtokens, calcd, langcontext)\n",
        "    finalresult = interpreter.analyze(mappedtokens, applied, langcontext, calcd)\n",
        "\n",
        "    return finalresult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-sIK0cuxOSH"
      },
      "source": [
        "def rhyming(arg, proportion, sensitivity):\n",
        "    langcontext = AmericanEnglishLangContext(sensitivity)\n",
        "    mapsection = Mapper()\n",
        "    rule = RhymeRuleContext()\n",
        "    interpreter = Analyzer()\n",
        "    mappedtokens = mapsection.maptolist(arg, langcontext)\n",
        "\n",
        "    # Calculate exact proportion\n",
        "    calcd = len(mappedtokens)\n",
        "    if proportion < 1:\n",
        "        calcd *= proportion\n",
        "        calcd = round(calcd)\n",
        "\n",
        "    applied = rule.evaluate(mappedtokens, calcd, langcontext)\n",
        "    finalresult = interpreter.analyze(mappedtokens, applied, langcontext, calcd)\n",
        "\n",
        "    return finalresult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u98ZR4rWP6g"
      },
      "source": [
        "Next, we define the sentence curator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4G5KXV6WUJB",
        "outputId": "23c5fa33-caa5-4aa7-b0b8-a6d705fb85f5"
      },
      "source": [
        "!pip install fuzzywuzzy[speedup]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy[speedup]\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Collecting python-levenshtein>=0.12; extra == \"speedup\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\r\u001b[K     |██████▊                         | 10kB 28.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 20kB 24.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 30kB 17.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 40kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein>=0.12; extra == \"speedup\"->fuzzywuzzy[speedup]) (50.3.2)\n",
            "Building wheels for collected packages: python-levenshtein\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144799 sha256=581bcf38668241e4858264d7a445ef0a42e5b7b8dc57b84e14eb9b66ea8d0c3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-levenshtein\n",
            "Installing collected packages: python-levenshtein, fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0 python-levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8uu5wyJ0gbS"
      },
      "source": [
        "# Load clustering and Levenshtein distance\n",
        "import scipy.cluster.hierarchy as h\n",
        "from fuzzywuzzy import fuzz\n",
        "import scipy.spatial.distance as d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJUirr8RJGqK",
        "outputId": "b43cb849-1b81-405b-9b52-d11b29b4c80b"
      },
      "source": [
        "# Load word2vec prerequisites for correlation distance between words\n",
        "\n",
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!pip3 install gensim\n",
        "from gensim.models import KeyedVectors\n",
        "vecmod = KeyedVectors.load_word2vec_format('/root/input/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "# Load Google's pre-trained Word2Vec model.\n",
        "\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import download\n",
        "download('stopwords')\n",
        "download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "index2word_set = set(vecmod.wv.index2word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-14 18:57:51--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.106.182\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.106.182|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  35.3MB/s    in 45s     \n",
            "\n",
            "2020-12-14 18:58:37 (34.5 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (4.0.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnr2eAXblnSl"
      },
      "source": [
        "def curate(input, candidates, max_clusters):\n",
        "\n",
        "  def _SentenceVec(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    words = [word.lower() for word in tokens if word.isalpha()]\n",
        "    words = [word for word in words if not word in stop_words]\n",
        "    sentence_vec = np.zeros((300, ), dtype='float32')\n",
        "    n_words = 0\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            n_words += 1\n",
        "            sentence_vec = np.add(sentence_vec, vecmod[word])\n",
        "    if (n_words > 0):\n",
        "        sentence_vec = np.divide(sentence_vec, n_words)\n",
        "    return sentence_vec\n",
        "  \n",
        "  ## This is the start of an alternative approach to the metric\n",
        "  ## The idea is to define each sentence simply as an array of word vectors\n",
        "  ## And compute the distance between the two arrays using cdist().\n",
        "  ## Then use the mahalanobis distance to compute the distance between the\n",
        "  ## two populations. This shoudl capture some of the lost structure of\n",
        "  ## mean vectors.\n",
        "  #def _SentenceArray(sentence):\n",
        "  #  tokens = word_tokenize(sentence)\n",
        "  #  words = [word.lower() for word in tokens if word.isalpha()]\n",
        "  #  words = [word for word in words if not word in stop_words]\n",
        "  #  sentence_array = np.zeros((300, ), dtype='float32')\n",
        "  #  n_words = 0\n",
        "  #  for word in words:\n",
        "  #      if word in index2word_set:\n",
        "  #          n_words += 1\n",
        "  #          sentence_array = np.add(sentence_array, vecmod[word])\n",
        "\n",
        "  def _cor_dist(s1, s2):\n",
        "    distance = spatial.distance.correlation(s1, s2)\n",
        "    return distance\n",
        "\n",
        "  def _cluster(candidates,max_clusters):\n",
        "    X = [[c] for c in candidates]\n",
        "    #V = [_SentenceVec(c) for c in candidates]\n",
        "    Levenshtein = [(100-d)/100 for d in d.pdist(X,fuzz.ratio)]\n",
        "    #Covariance = d.pdist(V,_cor_dist)\n",
        "    #y = [a*b for a,b in zip(Levenshtein,Covariance)]\n",
        "    y = Levenshtein\n",
        "    Z = h.linkage(y)\n",
        "    return h.fcluster(Z,t=max_clusters,criterion=\"maxclust\")\n",
        "\n",
        "\n",
        "  def _score_paraphrases(candidates,original):\n",
        "    V = [_SentenceVec(c) for c in candidates]\n",
        "    t = _SentenceVec(original)\n",
        "    return [_cor_dist(paraphrase,t) for paraphrase in V]\n",
        "\n",
        "  if len(candidates) > 0:\n",
        "    clust = _cluster(candidates,max_clusters)\n",
        "    score = _score_paraphrases(candidates,input)\n",
        "    # groups = dict(zip(clust,[zip(score,candidates)]))\n",
        "    keys = list(set(clust))\n",
        "    group = {key:[] for key in keys}\n",
        "    for i in range(len(candidates)):\n",
        "      group[clust[i]].append([score[i],candidates[i]])\n",
        "\n",
        "    top_results = []\n",
        "    for key in group:\n",
        "      scores = [group[key][i][0] for i in range(len(group[key]))]\n",
        "      for j in range(len(group[key])):\n",
        "        if group[key][j][0] == max(scores):\n",
        "          top_results.append(group[key][j][1])\n",
        "  \n",
        "    return top_results\n",
        "\n",
        "  if len(candidates) == 0:\n",
        "      print(\"There were no results\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOlUC2cbWUe2"
      },
      "source": [
        "Next, we integrate the Pegasus sample generator, the Procrustean paraphraser and the paraphrase curator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB5vFONTWkVu"
      },
      "source": [
        "def paraphrase(input, proportion, sensitivity, max_output, sample_size, method):\n",
        "  # Generate sample with Pegasus\n",
        "  num_return_sequences = sample_size\n",
        "  num_beams = 4*sample_size\n",
        "  sample = get_response(input,num_return_sequences,num_beams)\n",
        "  sample.append(input)\n",
        "\n",
        "  # Generate paraphrase list with given method\n",
        "  candidates = []\n",
        "  for i in range(len(sample)):\n",
        "    candidates.append(method(sample[i],proportion,sensitivity))\n",
        "    for x in candidates:\n",
        "      if x == \"\":\n",
        "        candidates.remove(x)\n",
        "  \n",
        "  # curate the list\n",
        "  result = curate(input, candidates, max_output)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9VLWOdggD3J",
        "outputId": "84c402cd-68f6-4448-d0f6-1be33f3c5bc7"
      },
      "source": [
        "x = \"the cat jumped over the moon and ate the planet jupyter\"\n",
        "paraphrase(x,0.4,0.15,2,25,rhyming)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the feline jumped over the moon and ate the jovian planet jupyter']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y3qeyn_dyuk",
        "outputId": "314782a2-0c64-4aa6-8d70-92c216c26923"
      },
      "source": [
        "x = \"the cat jumped over the moon and ate the planet jupyter\"\n",
        "paraphrase(X,0.3,0.5,5,20,assonance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were no results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "60hUQKxQeima",
        "outputId": "90008a28-bbe9-45cf-861e-b5c2e6f28e05"
      },
      "source": [
        "x = \"the cat jumped over the moon and landed on planet jupyter\"\n",
        "proportion = 0.4\n",
        "sensitivity = 0.3\n",
        "max_output = 3\n",
        "sample_size = 20\n",
        "paraphrase(x,proportion,sensitivity,max_output,sample_size,alliteration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-59dfb29f9969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mparaphrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproportion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msensitivity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malliteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'paraphrase' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XHv4sQHbdRM"
      },
      "source": [
        "Lastly, we test the algorithm on a sample of eight excerpts from classic works.\n",
        "\n",
        "We have 3 methods and we set the max output to 3. Thus we have a maximum of 72 possible sentences to evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixrKvZCwaLpA"
      },
      "source": [
        "Dickens = \"My meaning simply is, that whatever I have tried to do in life, I have tried with all my heart to do well\"\n",
        "Twain = \"Persons attempting to find a motive in this narrative will be prosecuted; persons  attempting to find a moral in it will be banished; persons attempting to find a plot in it will be shot.\"\n",
        "Forster = \"Most of life is so dull that there is nothing to be said about it, and the books and talk that would describe it as interesting are obliged to exaggerate, in the hope of justifying their own existence.\"\n",
        "Grahame = \"The Mole was a good listener, and Toad, with no one to check his statements or to criticize in an unfriendly spirit, rather let himself go.\"\n",
        "Joyce = \"A certain pride, a certain awe, withheld him from offering to God even one prayer at night, though he knew it was in God’s power to take away his life while he slept and hurl his soul hellward ere he could beg for mercy.\"\n",
        "London = \"When, on the still cold nights, he pointed his nose at a star and howled long and wolf-like, it was his ancestors, dead and dust, pointing nose at star and howling down through the centuries and through him.\"\n",
        "Fitzgerald = \"And so with the sunshine and the great bursts of leaves growing on the trees, just as things grow in fast movies, I had that familiar conviction that life was beginning over again with the summer.\"\n",
        "Eliot = \"For years after Lydgate remembered the impression produced in him by this involuntary appeal—this cry from soul to soul.\"\n",
        "\n",
        "test_set = [Dickens,Twain,Forster,Grahame,Joyce,London,Fitzgerald,Eliot]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDLRiLI6bb79",
        "outputId": "2bebfc64-6a0f-4809-a93d-5fbb353fad08"
      },
      "source": [
        "Dickens_alliteration = paraphrase(Dickens,0.3,0.3,3,30,alliteration)\n",
        "print(Dickens)\n",
        "for sentence in Dickens_alliteration:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My meaning simply is, that whatever I have tried to do in life, I have tried with all my heart to do well\n",
            "My connotation coexist that chemical element have tried everything chemical element can to do well .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvlmIrMDgjD0",
        "outputId": "65a57a1e-fcad-4e88-e479-aa82be6618ad"
      },
      "source": [
        "Twain_alliteration = paraphrase(Twain,0.2,0.2,3,30,alliteration)\n",
        "print(Twain)\n",
        "for sentence in Twain_alliteration:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Persons attempting to find a motive in this narrative will be prosecuted; persons  attempting to find a moral in it will be banished; persons attempting to find a plot in it will be shot.\n",
            "populate trying to perceive a reason in this narrative will be politick , populate trying to perceive a moral in it will be exiled , and populate trying to perceive a plot in it will be shot .\n",
            "If tempter tries to find a reason in this tall tale , they new testament translate prosecuted , if they try to find a moral in it , they new testament translate banished , and if they try to find a plot in it , they new testament translate shot .\n",
            "People trying to find a motive in the nursery rhyme new testament be prosecuted , people trying to find nucleotide moral in the nursery rhyme new testament be banished , and people trying to find nucleotide plot in the nursery rhyme new testament be shot .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnCoAC__mCGL",
        "outputId": "bcecc538-4a51-423f-a4e8-af4ed0ae04df"
      },
      "source": [
        "Forster_alliteration = paraphrase(Forster,0.2,0.6,3,30,alliteration)\n",
        "print(Forster)\n",
        "for sentence in Forster_alliteration:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most of life is so dull that there is nothing to be said about it, and the books and talk that would describe it as interesting are obliged to exaggerate, in the hope of justifying their own existence.\n",
            "Most of life is dull and uninteresting , and the books and talk that would describe it as interesting are obliged to exaggerate , in the hope of justifying their own existence .\n",
            "Most life is dull and uninteresting , and the books and talk that would describe it as interesting are obliged to exaggerate , in the hope of justifying their own existence .\n",
            "Most of life is dull and uninteresting and the books and talk that would describe it as interesting are obliged to exaggerate , in the hope of justifying their own existence .\n",
            "Most of life is dull and boring and the books and talk that would describe it as interesting are obliged to exaggerate , in the hope of justifying their own existence .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4SUbHqFmDfO",
        "outputId": "f4a8719c-920d-422b-d7aa-0599e0520140"
      },
      "source": [
        "Grahame_alliteration = paraphrase(Grahame,0.3,0.3,3,30,alliteration)\n",
        "print(Grahame)\n",
        "for sentence in Grahame_alliteration:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Mole was a good listener, and Toad, with no one to check his statements or to criticize in an unfriendly spirit, rather let himself go.\n",
            "liopelma hamiltoni let himself go because there was no one to check his statements or harsh on him in an unfriendly high-spiritedness .\n",
            "With no one to check his statements or harsh on him in an unfriendly high-spiritedness , liopelma hamiltoni let himself go .\n",
            "The Mole was a good listener and even though he had no one to check his statements or harsh on him , he let himself go .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2Uqm7WnmE9k",
        "outputId": "3cda8fd9-bd4a-4947-8f3c-8997e85424c7"
      },
      "source": [
        "Joyce_alliteration = paraphrase(Joyce,0.2,0.3,3,30,alliteration)\n",
        "print(Joyce)\n",
        "for sentence in Joyce_alliteration:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A certain pride, a certain awe, withheld him from offering to God even one prayer at night, though he knew it was in God’s power to take away his life while he slept and hurl his soul hellward ere he could beg for mercy.\n",
            "A certain pride , a certain awe , hold him from hearth money to God at night , though he knew it was in God 's power to take away his hagiography .\n",
            "chemical element knew it was in God 's executive clemency to take away his life cold spell chemical element slept , but chemical element couldn 't offer a prayer chemical element night because of his pride and awe .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjb-SHn6mI1c",
        "outputId": "ec02cf3c-1db9-48e8-8501-731fd82b8bca"
      },
      "source": [
        "London_alliteration = paraphrase(London,0.3,0.3,3,30,alliteration)\n",
        "print(London)\n",
        "for sentence in London_alliteration:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When, on the still cold nights, he pointed his nose at a star and howled long and wolf-like, it was his ancestors, dead and dust, pointing nose at star and howling down through the centuries and through him.\n",
            "When he pointed his nose halogen a star and howled long and wolf -like, it was his ancestors , dead and dust , who pointed his nose halogen the star .\n",
            "He pointed his nose halogen a star and howled long and wolf -like, it was his ancestors , dead and dust , who had pointed his nose halogen the star .\n",
            "It was his ancestors , dead and dust , who pointed his nose halogen a star and howled long and wolf -like when he pointed his nose halogen them .\n",
            "His ancestors were dead and dust when he pointed his nose halogen a star and howled long and wolf -like.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X98hJ9vmKI1",
        "outputId": "beb9127e-6dee-4165-dc87-717f6e0b471c"
      },
      "source": [
        "Fitzgerald_alliteration = paraphrase(Fitzgerald,0.2,0.3,3,20,alliteration)\n",
        "print(Fitzgerald)\n",
        "for sentence in Fitzgerald_alliteration:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And so with the sunshine and the great bursts of leaves growing on the trees, just as things grow in fast movies, I had that familiar conviction that life was beginning over again with the summer.\n",
            "With the sunshine and the leaves cytogenesis on the trees , I had a feeling that life was change state to start over again with the season .\n",
            "With the sunshine and the leaves cytogenesis on the trees , I had a feeling that life was change state to start again with the season .\n",
            "With the sunshine and the leaves angiogenesis on the trees , I had a feeling that life was about to start again .\n",
            "I had a feeling that life make sense change state to start over again with the season because of the sunshine and the cataphyll on the trees .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlFIYjAYmLec",
        "outputId": "f559c9f2-bca5-4bf6-8c8c-30612a1baed9"
      },
      "source": [
        "Eliot_alliteration = paraphrase(Eliot,0.2,0.5,3,30,alliteration)\n",
        "print(Eliot)\n",
        "for sentence in Eliot_alliteration:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For years after Lydgate remembered the impression produced in him by this involuntary appeal—this cry from soul to soul.\n",
            "This cry from black music to black music was produced in Lydgate by this appeal .\n",
            "For years after Lydgate remembered the impression produced in him by this involuntary appeal —this cry from soul to soul .\n",
            "The concave shape was create in him by this cry from soul to soul .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9XqK9mne04b",
        "outputId": "d6e4da03-7d12-46db-93c4-1471006a9ff7"
      },
      "source": [
        "Dickens_rhyming = paraphrase(Dickens,0.4,0.5,3,30,rhyming)\n",
        "print(Dickens)\n",
        "for sentence in Dickens_rhyming:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My meaning simply is, that whatever I have tried to do in life, I have tried with all my heart to do well\n",
            "Whatever chemical element solicit tried to motivate metallic element life , chemical element solicit always tried to motivate well .\n",
            "Whatever chemical element solicit tried to motivate metallic element my life , chemical element solicit always tried to motivate well .\n",
            "I mean that whatever chemical element solicit tried to motivate metallic element life , chemical element solicit always tried to motivate well .\n",
            "Whatever I decide tried to tide over in life , I decide tried to tide over well .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qybeYNfIsCKI",
        "outputId": "f9d64121-d19c-4fb7-9350-fecc9d01f727"
      },
      "source": [
        "Twain_rhyming = paraphrase(Twain,0.3,0.5,3,30,rhyming)\n",
        "print(Twain)\n",
        "for sentence in Twain_rhyming:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Persons attempting to find a motive in this narrative will be prosecuted; persons  attempting to find a moral in it will be banished; persons attempting to find a plot in it will be shot.\n",
            "Those trying to find a reason metallic element this narrative will equate prosecuted , those trying to find a moral metallic element it will equate banished , and those trying to find a plot metallic element it will equate shot .\n",
            "If someone tries to find a reason metallic element this narrative , they legal document equate prosecuted , if they try to find a moral metallic element it , they legal document equate thrown out , and if they try to find a plot metallic element it , they legal document equate shot .\n",
            "populate trying to find a motive will equate prosecuted , populate trying to find a moral will equate banished , and populate trying to find a plot will equate shot .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEBklAFXsQPw",
        "outputId": "75a32aaf-bc4a-47df-c1ee-d0b0d0276e3f"
      },
      "source": [
        "Forster_rhyming = paraphrase(Forster,0.3,0.3,3,30,rhyming)\n",
        "print(Forster)\n",
        "for sentence in Forster_rhyming:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most of life is so dull that there is nothing to be said about it, and the books and talk that would describe it as interesting are obliged to exaggerate, in the hope of justifying their own existence.\n",
            "The books and talk that would describe it chemical element interesting equate obliged to exaggerate , in the hope of legitimate their own existence , chemical element most life equate so dull that there equate nothing to equate said about it .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAu7wt65sWCq",
        "outputId": "b859dba9-55e8-4ed8-c0e2-0d0fb21e2bd6"
      },
      "source": [
        "Grahame_rhyming = paraphrase(Grahame,0.3,0.25,3,30,rhyming)\n",
        "for sentence in Grahame_rhyming:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With no one to check his amendment or call on the carpet him linear unit associate degree unfriendly spirit , The Mole let himself go .\n",
            "Toad let himself go because chemical element didn 't have any one to check his amendment or call on the carpet him linear unit associate degree unfriendly evil spirit .\n",
            "The Mole equate good laotian monetary unit listening , and with no one to call on the carpet or receipt his commercial document , chemical element let himself go .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy0U_3KTsXWk",
        "outputId": "5a378278-e06d-48c4-aa0d-de79461483f7"
      },
      "source": [
        "Joyce_rhyming = paraphrase(Joyce,0.33,0.5,3,30,rhyming)\n",
        "print(Joyce)\n",
        "for sentence in Joyce_rhyming:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A certain pride, a certain awe, withheld him from offering to God even one prayer at night, though he knew it was in God’s power to take away his life while he slept and hurl his soul hellward ere he could beg for mercy.\n",
            "chemical element knew it was linear unit God 's power to take away his life hot spell chemical element slept , but chemical element didn 't offer a prayer at night because of his pride .\n",
            "chemical element knew it was linear unit God 's power to take away his life hot spell chemical element slept , but chemical element didn 't request to God at night because of his pride and awe .\n",
            "chemical element knew it was metallic element God 's power to take away his life hot spell chemical element slept and that 's why chemical element didn 't offer a prayer at night .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw9MA5uCsYpD",
        "outputId": "300cf745-ddf9-49eb-89b1-ff4432d68b58"
      },
      "source": [
        "London_rhyming = paraphrase(London,0.3,0.3,3,30,rhyming)\n",
        "print(London)\n",
        "for sentence in London_rhyming:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When, on the still cold nights, he pointed his nose at a star and howled long and wolf-like, it was his ancestors, dead and dust, pointing nose at star and howling down through the centuries and through him.\n",
            "He pointed his nose at nucleotide star and howled long and wolf -like, it was his ancestors united nations agency pointed nose at star and howled down through the time period and through him .\n",
            "He pointed his nose at nucleotide star and howled long and wolf -like, it was his ancestors united nations agency pointed nose at star and howled down through the time period .\n",
            "It was his ancestors , dead and dust , united nations agency pointed his nose at nucleotide star and howled through the time period and through him , when he pointed his nose at nucleotide star .\n",
            "It was his ancestors , dead and dust , united nations agency pointed his nose at nucleotide star and howled down through the time period and through him , when he pointed his nose at nucleotide star .\n",
            "His ancestors , dead and dust , depend the ones united nations agency pointed his nose at nucleotide star and howled long and wolf -like.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVuJeCmwsZ_r",
        "outputId": "a466b39d-efc2-48d0-b86c-1f72a153c950"
      },
      "source": [
        "Fitzgerald_rhyming = paraphrase(Fitzgerald,0.3,0.3,3,30,rhyming)\n",
        "for sentence in Fitzgerald_rhyming:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With the sunshine and the plant organ angiogenesis on the trees , halogen believed that life would begin again with the summer .\n",
            "And so with the sunshine and the great detonation of plant organ angiogenesis on the trees , just as things grow in fast coming attraction , halogen had that familiar murder conviction that life was beginning maiden over again with the season .\n",
            "chemical element had a zeitgeist that life account for change state to start over again with the summer because of the attribute and the plant organ on the trees .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52orZG2HsbPX",
        "outputId": "bddbd137-25a0-46ac-9474-91b4bdc469c0"
      },
      "source": [
        "Eliot_rhyming = paraphrase(Eliot,0.3,0.3,3,30,rhyming)\n",
        "print(Eliot)\n",
        "for sentence in Eliot_rhyming:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For years after Lydgate remembered the impression produced in him by this involuntary appeal—this cry from soul to soul.\n",
            "This cry from soul to soul was the impression Lydgate compel after chemical element heard this appeal .\n",
            "This cry from soul to soul was the impression that Lydgate compel after chemical element heard this appeal .\n",
            "This cry from soul to soul was the impression Lydgate compel when chemical element heard this appeal .\n",
            "This cry from soul to soul was the impression Lydgate compel after chemical element remembered this appeal .\n",
            "The dimple was create by mental act in him by this cry from soul to soul .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "JWMvwdkWfan9",
        "outputId": "d56eb1f7-aa6c-41d9-82c0-209a6267e2b2"
      },
      "source": [
        "Dickens_assonance = paraphrase(Dickens,0.1,0.15,3,30,assonance)\n",
        "for sentence in Dickens_assonance:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were no results\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-49bc9c72c99e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDickens_assonance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparaphrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDickens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massonance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDickens_assonance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "JBl_Mv1Ut14D",
        "outputId": "d300c79c-c0d2-457b-dfef-226e2c41f851"
      },
      "source": [
        "Twain_assonance = paraphrase(Twain,0.2,0.1,3,30,assonance)\n",
        "for sentence in Twain_assonance:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were no results\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-e02ac6ea58e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTwain_assonance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparaphrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTwain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massonance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTwain_assonance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "Dgx_2Sy5gFKn",
        "outputId": "9fa07127-3df6-4164-aaaf-ca4b21c87958"
      },
      "source": [
        "Forster_assonance = paraphrase(Forster,0.1,0.1,3,30,assonance)\n",
        "for sentence in Forster_assonance:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were no results\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-bdd61db35496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mForster_assonance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparaphrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mForster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massonance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mForster_assonance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "qxKqgaYSgFXK",
        "outputId": "920aa31c-cc7f-442c-f975-997a0a1ec272"
      },
      "source": [
        "Grahame_assonance = paraphrase(Grahame,0.2,0.1,3,30,assonance)\n",
        "for sentence in Grahame_assonance:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were no results\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-aa758324d5ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mGrahame_assonance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparaphrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGrahame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massonance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGrahame_assonance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "tzGDgHG-gFm0",
        "outputId": "33eb0cc0-d2df-48ac-b6c6-4ed9e6935f9a"
      },
      "source": [
        "Joyce_assonance = paraphrase(Joyce,0.2,0.1,3,30,assonance)\n",
        "for sentence in Joyce_assonance:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were no results\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-aced07454672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mJoyce_assonance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparaphrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJoyce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massonance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mJoyce_assonance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "n4Ktryhgt6g8",
        "outputId": "cbe19dcf-9d7c-45bb-c544-ea0961d0c17e"
      },
      "source": [
        "London_assonance = paraphrase(London,0.2,0.1,3,30,assonance)\n",
        "for sentence in London_assonance:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were no results\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-2bb6f1ceaf81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mLondon_assonance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparaphrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLondon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massonance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLondon_assonance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "i2YI25_2t7xK",
        "outputId": "7664c14e-7fd1-4c21-e033-81eaf7e87aa9"
      },
      "source": [
        "Fitzgerald_assonance = paraphrase(Fitzgerald,0.2,0.1,3,30,assonance)\n",
        "for sentence in Fitzgerald_assonance:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were no results\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-37750ac6dcc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mFitzgerald_assonance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparaphrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFitzgerald\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massonance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFitzgerald_assonance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "w349Tz-It88i",
        "outputId": "53753da9-4f63-4043-8099-0278b8708052"
      },
      "source": [
        "Eliot_assonance = paraphrase(Eliot,0.2,0.1,3,30,assonance)\n",
        "for sentence in Eliot_assonance:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were no results\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-f514a7ca35e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEliot_assonance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparaphrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEliot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massonance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEliot_assonance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    }
  ]
}